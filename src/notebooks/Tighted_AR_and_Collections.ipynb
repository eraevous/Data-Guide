{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ How much revenue is locked in outstanding AR? \n",
    "<br> ---- sum total vs total payments\n",
    "<br>ðŸ”¹ Which claims remain unresolved longest? \n",
    "<br> ---- combine with transactions\n",
    "<br>ðŸ”¹ Which payers cause the most delays?\n",
    "<br> ---- combine with patient details\n",
    "<br>ðŸ”¹ Do certain procedures have longer collection times?\n",
    "<br> ---- combine with transactions\n",
    "<br>ðŸ”¹ Do older patients or certain insurance plans have slower payments?\n",
    "<br> ---- combine with patient details\n",
    "<br>ðŸ”¹ What % of AR is overdue? \n",
    "<br> ---- sum total vs total payments\n",
    "<br>ðŸ”¹ How likely is a bill in \"90+ Days\" to remain unpaid?\n",
    "<br>ðŸ”¹ Which insurance companies pay on time vs. delay payments?\n",
    "<br> ---- combine with patient details and insurance details\n",
    "<br>ðŸ”¹ How often do denied claims result in collection delays?\n",
    "<br>ðŸ”¹ Can we predict claims risk, optimize payment reminders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {\n",
    "    \"revenue_in_AR\": 0,\n",
    "    \"longest_claims\": 0,\n",
    "    \"delayed_payers\": 0,\n",
    "    \"procedure_collection_times\": 0,\n",
    "    \"slower_payments\": 0,\n",
    "    \"overdue_AR_percentage\": 0,\n",
    "    \"liklihood_of_default\": 0,\n",
    "    \"insurance_punctuality\": 0,\n",
    "    \"denied_claims_to_delays\": 0,\n",
    "    \"claims_risk\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Needed\n",
    "<br>ðŸ”¹ Aged AR (long form)<br>ðŸ”¹ Outstanding Claims<br>ðŸ”¹ Insurance Payments & Adjustments<br>ðŸ”¹ Processed Payments <br>ðŸ”¹ Statement Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import Levenshtein\n",
    "from itertools import combinations\n",
    "from scipy.stats import gmean\n",
    "import profiler as pf\n",
    "\n",
    "os.chdir('C:/Users/Admin/Documents/GitHub/Data-Guide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_date = pd.to_datetime('2025-02-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/Admin/Documents/GitHub/Data-Guide/data_pipeline/transformed_feb_18\" \n",
    "\n",
    "output_dir = \"C:/Users/Admin/Documents/GitHub/Data-Guide/data_pipeline/analyses_feb_18\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load the data\n",
    "csv_files = {\n",
    "    \"aged_AR\" : os.path.join(input_dir, \"transformed_aged_AR.csv\"),\n",
    "    \"aged_AR_long\" : os.path.join(input_dir, \"transformed_aged_AR_long.csv\"),\n",
    "    \"statement_submission\" : os.path.join(input_dir, \"transformed_statement_submission.csv\"),\n",
    "    \"integrated_payments\" : os.path.join(input_dir, \"transformed_integrated_payments.csv\"),\n",
    "    #\"billing_statement\" : os.path.join(input_dir, \"billing_statement_report.csv\"),\n",
    "    \"outstanding_claims\" : os.path.join(input_dir, \"transformed_outstanding_claims.csv\"),\n",
    "    # \"unresolved_claims\" : os.path.join(input_dir, \"unresolved_claims_report.csv\"),\n",
    "    #\"fee_schedule\" : os.path.join(input_dir, \"fee_schedule.csv\"),\n",
    "    #\"openings\" : os.path.join(input_dir,\"openings.csv\"),\n",
    "    #\"schedule\" : os.path.join(input_dir,\"schedule.csv\"),\n",
    "#    \"patient_details\" : os.path.join(input_dir, \"transformed_patient_details.csv\"),\n",
    "    \"active_patients\" : os.path.join(input_dir, \"transformed_active_patient_details.csv\"),\n",
    "    \"processed_payments\": os.path.join(input_dir, \"transformed_processed_payments.csv\"),\n",
    "    \"payments\": os.path.join(input_dir, \"transformed_payments.csv\"),\n",
    "    \"incurred_charges\": os.path.join(input_dir, \"transformed_incurred_charges.csv\"),\n",
    "    \"transaction_details\" : os.path.join(input_dir, \"transformed_transaction_details.csv\"),\n",
    "    # \"treatment_tracker\" : os.path.join(input_dir, \"ZR - Treatment Tracker.csv\"),\n",
    "    # \"merged_data\" : os.path.join(input_dir, \"merged_data.csv\"),\n",
    "    'carrier_decision_data' : os.path.join(input_dir, 'Carrier_Decision_Data.csv'),\n",
    "    'insurance_payment_metrics' : os.path.join(input_dir, 'insurance_payment_metrics.csv'),\n",
    "    \"financial_timeline\" : os.path.join(input_dir, \"financial_timeline.csv\"),\n",
    "    'time_to_payments' : os.path.join(input_dir, \"time_to_payments.csv\"),\n",
    "}\n",
    " # Load datasets\n",
    "dataframes = {dataset: pd.read_csv(file_path) for dataset, file_path in csv_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['aged_AR_long'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgedARVisualizer:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "        self.buckets = ['0-30', '31-60', '61-90', '91+']\n",
    "        self.df = self._reshape_data()\n",
    "\n",
    "    def _reshape_data(self):\n",
    "        \"\"\"\n",
    "        Reshape data so that each row represents a Responsible Party, with Amounts per bucket.\n",
    "        \"\"\"\n",
    "        pivot_df = self.df.pivot_table(index=['Responsible Party', 'Ascend Patient ID'], \n",
    "                                       columns='Bucket', values='Amount', aggfunc='sum').fillna(0)\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "        return pivot_df\n",
    "\n",
    "    def plot_subplots(self):\n",
    "        \"\"\"\n",
    "        Create a subplot for each Responsible Party.\n",
    "        \"\"\"\n",
    "        parties = self.df['Responsible Party'].unique()\n",
    "        fig, axes = plt.subplots(len(parties), 1, figsize=(12, 6 * len(parties)), sharex=True)\n",
    "        \n",
    "        if len(parties) == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable\n",
    "        \n",
    "        for ax, party in zip(axes, parties):\n",
    "            subset = self.df[self.df['Responsible Party'] == party]\n",
    "            parallel_coordinates(subset, class_column='Responsible Party', cols=self.buckets, ax=ax, colormap='tab10')\n",
    "            ax.set_title(f'Parallel Coordinates Plot for {party}')\n",
    "            ax.set_ylabel(\"Amount\")\n",
    "            ax.legend().remove()\n",
    "        \n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_colored(self):\n",
    "        \"\"\"\n",
    "        Create a single Parallel Coordinates plot with color-coded Responsible Party (excluding Total & Write-Off).\n",
    "        \"\"\"\n",
    "        filtered_df = self.df[~self.df['Responsible Party'].isin(['Total', 'Write-Off'])]\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        parallel_coordinates(filtered_df, class_column='Responsible Party', cols=self.buckets, colormap='tab10')\n",
    "        plt.title(\"Parallel Coordinates Plot by Responsible Party\")\n",
    "        plt.ylabel(\"Amount\")\n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.legend(title='Responsible Party')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aged_ar_long = dataframes['aged_AR_long'].copy()\n",
    "aged_ar_long.value_counts('Bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_mapping = {\n",
    "    '0-30' : (pull_date - pd.DateOffset(days=30), pull_date),\n",
    "    '31-60' : (pull_date - pd.DateOffset(days=60), pull_date - pd.DateOffset(days=31)),\n",
    "    '61-90' : (pull_date - pd.DateOffset(days=90), pull_date - pd.DateOffset(days=61)),\n",
    "    '91+' : (pd.to_datetime('2000-01-01'), pull_date - pd.DateOffset(days=91)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aged_ar_long['timespan'] = aged_ar_long['Bucket'].map(ar_mapping)\n",
    "ar_long = aged_ar_long.loc[(aged_ar_long['Amount'] != 0) & (aged_ar_long['Responsible Party'] != 'Total')].copy()\n",
    "ar_long['timespan_start'] = ar_long['timespan'].apply(lambda x: x[0])\n",
    "ar_long['timespan_end'] = ar_long['timespan'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_long.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgedARVisualizer:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "        self.buckets = ['0-30', '31-60', '61-90', '91+']\n",
    "        \n",
    "        print(\"ðŸ” Checking initial DataFrame columns:\", self.df.columns)\n",
    "        \n",
    "        self.df = self._reshape_data()\n",
    "\n",
    "    def _reshape_data(self):\n",
    "        \"\"\"\n",
    "        Reshape data so that each row represents a Responsible Party, with Amounts per bucket.\n",
    "        \"\"\"\n",
    "        pivot_df = self.df.pivot_table(index=['Responsible Party', 'Ascend Patient ID'], \n",
    "                                       columns='Bucket', values='Amount', aggfunc='sum').fillna(0)\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "\n",
    "        print(\"âœ… Pivoted DataFrame columns:\", pivot_df.columns)  # Debugging\n",
    "\n",
    "        return pivot_df\n",
    "\n",
    "    def plot_subplots(self):\n",
    "        \"\"\"\n",
    "        Create a subplot for each Responsible Party.\n",
    "        \"\"\"\n",
    "        parties = self.df['Responsible Party'].unique()\n",
    "        fig, axes = plt.subplots(len(parties), 1, figsize=(12, 6 * len(parties)), sharex=True)\n",
    "        \n",
    "        if len(parties) == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable\n",
    "        \n",
    "        for ax, party in zip(axes, parties):\n",
    "            subset = self.df[self.df['Responsible Party'] == party]\n",
    "            \n",
    "            print(f\"ðŸ” Debugging: Subset for {party} -> Columns:\", subset.columns)  # Debugging\n",
    "            \n",
    "            if 'Responsible Party' not in subset.columns:\n",
    "                print(f\"âŒ 'Responsible Party' is missing from subset for {party}!\")\n",
    "            \n",
    "            parallel_coordinates(subset, class_column='Responsible Party', cols=self.buckets, ax=ax, colormap='tab10')\n",
    "            ax.set_title(f'Parallel Coordinates Plot for {party}')\n",
    "            ax.set_ylabel(\"Amount\")\n",
    "            ax.legend().remove()\n",
    "        \n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_colored(self):\n",
    "        \"\"\"\n",
    "        Create a single Parallel Coordinates plot with color-coded Responsible Party (excluding Total & Write-Off).\n",
    "        \"\"\"\n",
    "        filtered_df = self.df[~self.df['Responsible Party'].isin(['Total', 'Write-Off'])]\n",
    "        \n",
    "        print(\"ðŸ” Filtered DataFrame (plot_colored) columns:\", filtered_df.columns)  # Debugging\n",
    "        \n",
    "        if 'Responsible Party' not in filtered_df.columns:\n",
    "            print(\"âŒ 'Responsible Party' is missing from filtered_df!\")\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        parallel_coordinates(filtered_df, class_column='Responsible Party', cols=self.buckets, colormap='tab10', alpha=0.5)\n",
    "        plt.title(\"Parallel Coordinates Plot by Responsible Party\")\n",
    "        plt.ylabel(\"Amount\")\n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.legend(title='Responsible Party')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df = pd.DataFrame(aged_ar_long)\n",
    "    \n",
    "visualizer = AgedARVisualizer(df)\n",
    "visualizer.plot_subplots()  # Option 1: Subplots per Responsible Party\n",
    "visualizer.plot_colored()   # Option 2: Color-coded plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Parallel Coordinates plot of Amount by Bucket colored by Responsible Party\n",
    "\n",
    "# Select relevant columns for the plot\n",
    "plot_data = aged_ar_long[['Amount', 'Bucket', 'Responsible Party']]\n",
    "\n",
    "# Create the parallel coordinates plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "parallel_coordinates(plot_data, class_column='Responsible Party', cols=['Amount'], color=plt.cm.Set1.colors)\n",
    "plt.title('Parallel Coordinates Plot of Amount by Bucket colored by Responsible Party')\n",
    "plt.xlabel('Attributes')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures = dataframes['financial_timeline'].query('Category == \"Procedures\"').copy()\n",
    "procedures['Date'] = pd.to_datetime(procedures['Date'])\n",
    "procedures.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the join\n",
    "merged_df = procedures.merge(ar_long, on=\"Ascend Patient ID\", how=\"inner\")\n",
    "\n",
    "# Filtering to ensure the procedure date falls within the AR timespan\n",
    "matched_df = merged_df[\n",
    "    (merged_df[\"Date\"] >= merged_df[\"timespan_start\"]) & (merged_df[\"Date\"] <= merged_df[\"timespan_end\"])\n",
    "].drop_duplicates().drop(columns=[\"timespan\", \"timespan_start\", \"timespan_end\", 'Category']).sort_values(['Ascend Patient ID', \"Date\", \"Proc. Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df.loc[(matched_df['Bucket'] != \"91+\") & (matched_df['Responsible Party'] == \"Guarantor\")].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_payments = dataframes['time_to_payments'].copy()\n",
    "time_to_payments['Date'] = pd.to_datetime(time_to_payments['Date'])\n",
    "\n",
    "time_to_payments['total_paid'] = time_to_payments['Insurance Payment Amount'] + time_to_payments['Guarantor Payment Amount'] + time_to_payments['Adjustment Payment Amount']\n",
    "\n",
    "time_to_payments['remaining_balance'] = time_to_payments['Value'] - time_to_payments['total_paid']\n",
    "time_to_payments = time_to_payments.loc[time_to_payments['remaining_balance'] != 0]\n",
    "time_to_payments['row_id'] = time_to_payments.index\n",
    "time_to_payments.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_to_payments['temp'] = time_to_payments['remaining_balance'] * -1\n",
    "balanced = time_to_payments.merge(time_to_payments, on='Ascend Patient ID', suffixes=('_1', '_2'))\n",
    "drop_ind = balanced.loc[(balanced['remaining_balance_1'] == balanced['remaining_balance_2'] * -1), ['Ascend Patient ID', 'Date_1', 'Date_2', 'remaining_balance_1', 'remaining_balance_2', 'row_id_1', 'row_id_2']]['row_id_1'].values\n",
    "drop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_payments.loc[(~time_to_payments['row_id'].isin(drop_ind))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR Collection Efficiency & Write-Off Risk<br>\n",
    "ðŸ“Œ Goal: Identify delinquent accounts, aging trends, and recovery probability.<br>\n",
    "âœ… Steps:<br>\n",
    "\n",
    "Calculate % of AR in each aging bucket (30, 60, 90, 120+ days).<br>\n",
    "Rank patients & insurance plans by collections risk.<br>\n",
    "Identify patterns in write-offs vs. successful collections.<br>\n",
    "âœ… Datasets Used:<br>\n",
    "Aged AR (long form), Outstanding Claims, Processed Payments<br>\n",
    "ðŸ“Œ Business Impact:<br>\n",
    "ðŸš€ Reduces bad debt write-offs.<br>\n",
    "ðŸš€ Optimizes collection strategy based on payer trends.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR Aging Forecasting & Collections Prioritization <br>\n",
    "ðŸ“Œ Goal: Predict which AR accounts are likely to default.<br>\n",
    "âœ… Approach:<br>\n",
    "\n",
    "Use time series forecasting (Prophet, ARIMA, LSTMs) to predict AR trends.<br>\n",
    "Train a classification model to rank overdue accounts by likelihood of non-payment.<br>\n",
    "âœ… Datasets Used:<br>\n",
    "Aged AR, Processed Payments, Financial Timeline<br>\n",
    "ðŸ“Œ Business Impact:<br>\n",
    "ðŸš€ Reduces bad debt by prioritizing high-risk accounts.<br>\n",
    "ðŸš€ Improves long-term financial planning.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
