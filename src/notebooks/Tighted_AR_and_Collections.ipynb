{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîπ How much revenue is locked in outstanding AR? \n",
    "<br> ---- sum total vs total payments\n",
    "<br>üîπ Which claims remain unresolved longest? \n",
    "<br> ---- combine with transactions\n",
    "<br>üîπ Which payers cause the most delays?\n",
    "<br> ---- combine with patient details\n",
    "<br>üîπ Do certain procedures have longer collection times?\n",
    "<br> ---- combine with transactions\n",
    "<br>üîπ Do older patients or certain insurance plans have slower payments?\n",
    "<br> ---- combine with patient details\n",
    "<br>üîπ What % of AR is overdue? \n",
    "<br> ---- sum total vs total payments\n",
    "<br>üîπ How likely is a bill in \"90+ Days\" to remain unpaid?\n",
    "<br>üîπ Which insurance companies pay on time vs. delay payments?\n",
    "<br> ---- combine with patient details and insurance details\n",
    "<br>üîπ How often do denied claims result in collection delays?\n",
    "<br>üîπ Can we predict claims risk, optimize payment reminders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {\n",
    "    \"revenue_in_AR\": 0,\n",
    "    \"longest_claims\": 0,\n",
    "    \"delayed_payers\": 0,\n",
    "    \"procedure_collection_times\": 0,\n",
    "    \"slower_payments\": 0,\n",
    "    \"overdue_AR_percentage\": 0,\n",
    "    \"liklihood_of_default\": 0,\n",
    "    \"insurance_punctuality\": 0,\n",
    "    \"denied_claims_to_delays\": 0,\n",
    "    \"claims_risk\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Needed\n",
    "<br>üîπ Aged AR (long form)<br>üîπ Outstanding Claims<br>üîπ Insurance Payments & Adjustments<br>üîπ Processed Payments <br>üîπ Statement Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import Levenshtein\n",
    "from itertools import combinations\n",
    "from scipy.stats import gmean\n",
    "import profiler as pf\n",
    "\n",
    "os.chdir('C:/Users/Admin/Documents/GitHub/Data-Guide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_date = pd.to_datetime('2025-02-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/Admin/Documents/GitHub/Data-Guide/data_pipeline/transformed_feb_18\" \n",
    "\n",
    "output_dir = \"C:/Users/Admin/Documents/GitHub/Data-Guide/data_pipeline/analyses_feb_18\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load the data\n",
    "csv_files = {\n",
    "    \"aged_AR\" : os.path.join(input_dir, \"transformed_aged_AR.csv\"),\n",
    "    \"aged_AR_long\" : os.path.join(input_dir, \"transformed_aged_AR_long.csv\"),\n",
    "    \"statement_submission\" : os.path.join(input_dir, \"transformed_statement_submission.csv\"),\n",
    "    \"integrated_payments\" : os.path.join(input_dir, \"transformed_integrated_payments.csv\"),\n",
    "    #\"billing_statement\" : os.path.join(input_dir, \"billing_statement_report.csv\"),\n",
    "    \"outstanding_claims\" : os.path.join(input_dir, \"transformed_outstanding_claims.csv\"),\n",
    "    # \"unresolved_claims\" : os.path.join(input_dir, \"unresolved_claims_report.csv\"),\n",
    "    #\"fee_schedule\" : os.path.join(input_dir, \"fee_schedule.csv\"),\n",
    "    #\"openings\" : os.path.join(input_dir,\"openings.csv\"),\n",
    "    #\"schedule\" : os.path.join(input_dir,\"schedule.csv\"),\n",
    "#    \"patient_details\" : os.path.join(input_dir, \"transformed_patient_details.csv\"),\n",
    "    \"active_patients\" : os.path.join(input_dir, \"transformed_active_patient_details.csv\"),\n",
    "    \"processed_payments\": os.path.join(input_dir, \"transformed_processed_payments.csv\"),\n",
    "    \"payments\": os.path.join(input_dir, \"transformed_payments.csv\"),\n",
    "    \"incurred_charges\": os.path.join(input_dir, \"transformed_incurred_charges.csv\"),\n",
    "    \"transaction_details\" : os.path.join(input_dir, \"transformed_transaction_details.csv\"),\n",
    "    # \"treatment_tracker\" : os.path.join(input_dir, \"ZR - Treatment Tracker.csv\"),\n",
    "    # \"merged_data\" : os.path.join(input_dir, \"merged_data.csv\"),\n",
    "    'carrier_decision_data' : os.path.join(input_dir, 'Carrier_Decision_Data.csv'),\n",
    "    'insurance_payment_metrics' : os.path.join(input_dir, 'insurance_payment_metrics.csv'),\n",
    "    \"financial_timeline\" : os.path.join(input_dir, \"financial_timeline.csv\"),\n",
    "    'time_to_payments' : os.path.join(input_dir, \"time_to_payments.csv\"),\n",
    "}\n",
    " # Load datasets\n",
    "dataframes = {dataset: pd.read_csv(file_path) for dataset, file_path in csv_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['aged_AR_long'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgedARVisualizer:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "        self.buckets = ['0-30', '31-60', '61-90', '91+']\n",
    "        self.df = self._reshape_data()\n",
    "\n",
    "    def _reshape_data(self):\n",
    "        \"\"\"\n",
    "        Reshape data so that each row represents a Responsible Party, with Amounts per bucket.\n",
    "        \"\"\"\n",
    "        pivot_df = self.df.pivot_table(index=['Responsible Party', 'Ascend Patient ID'], \n",
    "                                       columns='Bucket', values='Amount', aggfunc='sum').fillna(0)\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "        return pivot_df\n",
    "\n",
    "    def plot_subplots(self):\n",
    "        \"\"\"\n",
    "        Create a subplot for each Responsible Party.\n",
    "        \"\"\"\n",
    "        parties = self.df['Responsible Party'].unique()\n",
    "        fig, axes = plt.subplots(len(parties), 1, figsize=(12, 6 * len(parties)), sharex=True)\n",
    "        \n",
    "        if len(parties) == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable\n",
    "        \n",
    "        for ax, party in zip(axes, parties):\n",
    "            subset = self.df[self.df['Responsible Party'] == party]\n",
    "            parallel_coordinates(subset, class_column='Responsible Party', cols=self.buckets, ax=ax, colormap='tab10')\n",
    "            ax.set_title(f'Parallel Coordinates Plot for {party}')\n",
    "            ax.set_ylabel(\"Amount\")\n",
    "            ax.legend().remove()\n",
    "        \n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_colored(self):\n",
    "        \"\"\"\n",
    "        Create a single Parallel Coordinates plot with color-coded Responsible Party (excluding Total & Write-Off).\n",
    "        \"\"\"\n",
    "        filtered_df = self.df[~self.df['Responsible Party'].isin(['Total', 'Write-Off'])]\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        parallel_coordinates(filtered_df, class_column='Responsible Party', cols=self.buckets, colormap='tab10')\n",
    "        plt.title(\"Parallel Coordinates Plot by Responsible Party\")\n",
    "        plt.ylabel(\"Amount\")\n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.legend(title='Responsible Party')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aged_ar_long = dataframes['aged_AR_long'].copy()\n",
    "aged_ar_long.value_counts('Bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_mapping = {\n",
    "    '0-30' : (pull_date - pd.DateOffset(days=30), pull_date),\n",
    "    '31-60' : (pull_date - pd.DateOffset(days=60), pull_date - pd.DateOffset(days=31)),\n",
    "    '61-90' : (pull_date - pd.DateOffset(days=90), pull_date - pd.DateOffset(days=61)),\n",
    "    '91+' : (pd.to_datetime('2000-01-01'), pull_date - pd.DateOffset(days=91)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aged_ar_long['timespan'] = aged_ar_long['Bucket'].map(ar_mapping)\n",
    "ar_long = aged_ar_long.loc[(aged_ar_long['Amount'] != 0) & (aged_ar_long['Responsible Party'] != 'Total')].copy()\n",
    "ar_long['timespan_start'] = ar_long['timespan'].apply(lambda x: x[0])\n",
    "ar_long['timespan_end'] = ar_long['timespan'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_long.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgedARVisualizer:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "        self.buckets = ['0-30', '31-60', '61-90', '91+']\n",
    "        \n",
    "        print(\"üîç Checking initial DataFrame columns:\", self.df.columns)\n",
    "        \n",
    "        self.df = self._reshape_data()\n",
    "\n",
    "    def _reshape_data(self):\n",
    "        \"\"\"\n",
    "        Reshape data so that each row represents a Responsible Party, with Amounts per bucket.\n",
    "        \"\"\"\n",
    "        pivot_df = self.df.pivot_table(index=['Responsible Party', 'Ascend Patient ID'], \n",
    "                                       columns='Bucket', values='Amount', aggfunc='sum').fillna(0)\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "\n",
    "        print(\"‚úÖ Pivoted DataFrame columns:\", pivot_df.columns)  # Debugging\n",
    "\n",
    "        return pivot_df\n",
    "\n",
    "    def plot_subplots(self):\n",
    "        \"\"\"\n",
    "        Create a subplot for each Responsible Party.\n",
    "        \"\"\"\n",
    "        parties = self.df['Responsible Party'].unique()\n",
    "        fig, axes = plt.subplots(len(parties), 1, figsize=(12, 6 * len(parties)), sharex=True)\n",
    "        \n",
    "        if len(parties) == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable\n",
    "        \n",
    "        for ax, party in zip(axes, parties):\n",
    "            subset = self.df[self.df['Responsible Party'] == party]\n",
    "            \n",
    "            print(f\"üîç Debugging: Subset for {party} -> Columns:\", subset.columns)  # Debugging\n",
    "            \n",
    "            if 'Responsible Party' not in subset.columns:\n",
    "                print(f\"‚ùå 'Responsible Party' is missing from subset for {party}!\")\n",
    "            \n",
    "            parallel_coordinates(subset, class_column='Responsible Party', cols=self.buckets, ax=ax, colormap='tab10')\n",
    "            ax.set_title(f'Parallel Coordinates Plot for {party}')\n",
    "            ax.set_ylabel(\"Amount\")\n",
    "            ax.legend().remove()\n",
    "        \n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_colored(self):\n",
    "        \"\"\"\n",
    "        Create a single Parallel Coordinates plot with color-coded Responsible Party (excluding Total & Write-Off).\n",
    "        \"\"\"\n",
    "        filtered_df = self.df[~self.df['Responsible Party'].isin(['Total', 'Write-Off'])]\n",
    "        \n",
    "        print(\"üîç Filtered DataFrame (plot_colored) columns:\", filtered_df.columns)  # Debugging\n",
    "        \n",
    "        if 'Responsible Party' not in filtered_df.columns:\n",
    "            print(\"‚ùå 'Responsible Party' is missing from filtered_df!\")\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        parallel_coordinates(filtered_df, class_column='Responsible Party', cols=self.buckets, colormap='tab10', alpha=0.5)\n",
    "        plt.title(\"Parallel Coordinates Plot by Responsible Party\")\n",
    "        plt.ylabel(\"Amount\")\n",
    "        plt.xlabel(\"Aging Buckets\")\n",
    "        plt.legend(title='Responsible Party')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df = pd.DataFrame(aged_ar_long)\n",
    "    \n",
    "visualizer = AgedARVisualizer(df)\n",
    "visualizer.plot_subplots()  # Option 1: Subplots per Responsible Party\n",
    "visualizer.plot_colored()   # Option 2: Color-coded plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Parallel Coordinates plot of Amount by Bucket colored by Responsible Party\n",
    "\n",
    "# Select relevant columns for the plot\n",
    "plot_data = aged_ar_long[['Amount', 'Bucket', 'Responsible Party']]\n",
    "\n",
    "# Create the parallel coordinates plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "parallel_coordinates(plot_data, class_column='Responsible Party', cols=['Amount'], color=plt.cm.Set1.colors)\n",
    "plt.title('Parallel Coordinates Plot of Amount by Bucket colored by Responsible Party')\n",
    "plt.xlabel('Attributes')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures = dataframes['financial_timeline'].query('Category == \"Procedures\"').copy()\n",
    "procedures['Date'] = pd.to_datetime(procedures['Date'])\n",
    "procedures.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the join\n",
    "merged_df = procedures.merge(ar_long, on=\"Ascend Patient ID\", how=\"inner\")\n",
    "\n",
    "# Filtering to ensure the procedure date falls within the AR timespan\n",
    "matched_df = merged_df[\n",
    "    (merged_df[\"Date\"] >= merged_df[\"timespan_start\"]) & (merged_df[\"Date\"] <= merged_df[\"timespan_end\"])\n",
    "].drop_duplicates().drop(columns=[\"timespan\", \"timespan_start\", \"timespan_end\", 'Category']).sort_values(['Ascend Patient ID', \"Date\", \"Proc. Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df.loc[(matched_df['Bucket'] != \"91+\") & (matched_df['Responsible Party'] == \"Guarantor\")].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_payments = dataframes['time_to_payments'].copy()\n",
    "time_to_payments['Date'] = pd.to_datetime(time_to_payments['Date'])\n",
    "\n",
    "time_to_payments['total_paid'] = time_to_payments['Insurance Payment Amount'] + time_to_payments['Guarantor Payment Amount'] + time_to_payments['Adjustment Payment Amount']\n",
    "\n",
    "time_to_payments['remaining_balance'] = time_to_payments['Value'] - time_to_payments['total_paid']\n",
    "time_to_payments = time_to_payments.loc[time_to_payments['remaining_balance'] != 0]\n",
    "time_to_payments['row_id'] = time_to_payments.index\n",
    "time_to_payments.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_to_payments['temp'] = time_to_payments['remaining_balance'] * -1\n",
    "balanced = time_to_payments.merge(time_to_payments, on='Ascend Patient ID', suffixes=('_1', '_2'))\n",
    "drop_ind = balanced.loc[(balanced['remaining_balance_1'] == balanced['remaining_balance_2'] * -1), ['Ascend Patient ID', 'Date_1', 'Date_2', 'remaining_balance_1', 'remaining_balance_2', 'row_id_1', 'row_id_2']]['row_id_1'].values\n",
    "drop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_payments.loc[(~time_to_payments['row_id'].isin(drop_ind))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR Collection Efficiency & Write-Off Risk<br>\n",
    "üìå Goal: Identify delinquent accounts, aging trends, and recovery probability.<br>\n",
    "‚úÖ Steps:<br>\n",
    "\n",
    "Calculate % of AR in each aging bucket (30, 60, 90, 120+ days).<br>\n",
    "Rank patients & insurance plans by collections risk.<br>\n",
    "Identify patterns in write-offs vs. successful collections.<br>\n",
    "‚úÖ Datasets Used:<br>\n",
    "Aged AR (long form), Outstanding Claims, Processed Payments<br>\n",
    "üìå Business Impact:<br>\n",
    "üöÄ Reduces bad debt write-offs.<br>\n",
    "üöÄ Optimizes collection strategy based on payer trends.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR Aging Forecasting & Collections Prioritization <br>\n",
    "üìå Goal: Predict which AR accounts are likely to default.<br>\n",
    "‚úÖ Approach:<br>\n",
    "\n",
    "Use time series forecasting (Prophet, ARIMA, LSTMs) to predict AR trends.<br>\n",
    "Train a classification model to rank overdue accounts by likelihood of non-payment.<br>\n",
    "‚úÖ Datasets Used:<br>\n",
    "Aged AR, Processed Payments, Financial Timeline<br>\n",
    "üìå Business Impact:<br>\n",
    "üöÄ Reduces bad debt by prioritizing high-risk accounts.<br>\n",
    "üöÄ Improves long-term financial planning.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
