{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import Levenshtein\n",
    "from itertools import combinations\n",
    "from scipy.stats import gmean\n",
    "\n",
    "os.chdir('C:/Users/Admin/Documents/GitHub/Data-Guide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_timestamps(df, column, in_milliseconds=True):\n",
    "    \"\"\"\n",
    "    Convert Unix timestamps in a specified column to datetime.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column containing Unix timestamps.\n",
    "        in_milliseconds (bool): Whether the timestamps are in milliseconds. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the converted datetime column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        factor = 1000 if in_milliseconds else 1\n",
    "        coll = pd.to_datetime(df[column] / factor, unit='s', errors='coerce')\n",
    "        print(f\"Successfully converted {column} to datetime.\")\n",
    "    except Exception as e:\n",
    "        coll = df[column]\n",
    "        print(f\"Error converting column {column}: {e}\")\n",
    "    return coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_currency(df, column):\n",
    "    \"\"\"\n",
    "    Placeholder for currency treatment logic.\n",
    "    \"\"\"\n",
    "    coll = pd.to_numeric(df[column].replace(r'[\\$,]', '', regex=True).replace('-', np.nan), errors='coerce').astype(float)\n",
    "    coll = coll.fillna(0)\n",
    "    return coll    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure_map = {\n",
    "    \"Crowns\": [\n",
    "        \"Crown - 3/4 porcelain/ceramic\", \"Full Cast HNM Crown\", \"Full Porcelain/Ceramic Crown\",\n",
    "        \"Implant supported crown - porcelain fused to high noble alloys\",\n",
    "        \"Porcelain/HNM Crown\", \"Porcelain/HNM Pontic\", \"Porcelain/Noble Crown\",\n",
    "        \"Retainer crown - porcelain fused to high noble metal\"\n",
    "    ],\n",
    "    \n",
    "    \"Prophies\": [\"Prophylaxis - Adult\", \"Prophylaxis - Child\"],\n",
    "    \n",
    "    \"Fillings\": [\n",
    "        \"Anterior Resin Composite 1s\", \"Anterior Resin Composite 2s\", \"Anterior Resin Composite 3s\", \n",
    "        \"Anterior Resin Composite 4+s\", \"Posterior Resin Composite 1s\", \"Posterior Resin Composite 2s\", \n",
    "        \"Posterior Resin Composite 3s\", \"Posterior Resin Composite 4+s\"\n",
    "    ],\n",
    "    \n",
    "    \"Imaging\": [\n",
    "        \"2D Oral/Facial Photo Images\", \"Bitewing Four Images\", \"Bitewing Single Image\", \"Bitewing Two Images\",\n",
    "        \"Intraoral â comprehensive series of radiographic images\", \"Intraoral Periapical Add'l\", \n",
    "        \"Intraoral Periapical Images\", \"Panoramic Image\"\n",
    "    ],\n",
    "    \n",
    "    \"Evaluations\": [\n",
    "        \"Comprehensive Evaluation\", \"Periodic Evaluation\", \"Limited Evaluation\", \"Re-eval - Post-op Office Visit\",\n",
    "        \"Periodontal Evaluation\"\n",
    "    ],\n",
    "    \n",
    "    \"SRP\": [\"Scaling & Root Planing (1-3)\", \"Scaling & Root Planing (4-8)\"],\n",
    "    \n",
    "    \"Perio Maintenance\": [\"Periodontal Maintenance\", \"Scaling in presence of generalized gingival inflammation, full mouth\"],\n",
    "    \n",
    "    \"Appliance\": [\n",
    "        \"Occlusal guard - hard appliance, full arch\", \"Orthodontic Retention\", \n",
    "        \"Replacement of lost or broken retainer â mandibular\", \"Re-cement or re-bond fixed retainer â maxillary\",\n",
    "        \"Re-cement or re-bond crown\", \"Recement/bnd inlay/onlay/part\", \"Recemnt/bnd cast/prefab pst/cor\"\n",
    "    ],\n",
    "    \n",
    "    \"Other\": [\n",
    "        \"Bone Replacement Graft\", \"Palliative treatment of dental pain â per visit\",\n",
    "        \"Teeth White - In Office\", \"Teeth White - Take Home\", \"Topical Applic Fluoride Varnish\", \n",
    "        \"Topical Application of Fluoride\", \"Sealant\", \"StellaLife Gel\", \"StellaLife Rinse\",\n",
    "        \"Removal of fixed orthodontic appliances for reasons other than completion of treatment\"\n",
    "    ],\n",
    "    \n",
    "    \"Scheduling\": [\"Cancelled Appointment\", \"Late cancellation fee\"],\n",
    "    \n",
    "    # ❓ Unsure categories (need clarification)\n",
    "    \"Misc\": [\n",
    "        \"Dental Wellness Plan\", \"Diagnostic/Study Models\", \"Editorial change to the descriptor\", \n",
    "        \"Misc Invoice\", \"Routine Extraction ❓ (belongs under surgery?)\", \n",
    "        \"Remove Coronal Remnants - primary tooth ❓\", \"Limited Occlusal Adjustment ❓\",\n",
    "        \"External Bleaching-Office-Arch ❓ (Cosmetic?)\"\n",
    "    ],\n",
    "    \n",
    "    \"Dentures & Partials\": [\n",
    "        \"Interim Lower Partial Denture\", \"Interim Upper Partial Denture\", \"Lower Partial w/ Resin Base\"\n",
    "    ],\n",
    "    \n",
    "    \"Buildups & Adjustments\": [\n",
    "        \"Core Buildup w/ Any Pins\", \"Re-cement or re-bond crown\", \"Re-cement or re-bond fixed retainer â maxillary\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/Admin/Documents/GitHub/Data-Guide/data_pipeline/pull_feb_18\" \n",
    "\n",
    "output_dir = \"C:/Users/Admin/Documents/GitHub/Data-Guide/data_pipeline/transformed_feb_18\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load the data\n",
    "csv_files = {\n",
    "        \"aged_AR\" : os.path.join(input_dir, \"aged_ar_report.csv\"),\n",
    "        \"statement_submission\" : os.path.join(input_dir, \"statement_submission_report.csv\"),\n",
    "        \"integrated_payments\" : os.path.join(input_dir, \"integrated_payments_report.csv\"),\n",
    "        #\"billing_statement\" : os.path.join(input_dir, \"billing_statement_report.csv\"),\n",
    "        \"outstanding_claims\" : os.path.join(input_dir, \"outstanding_claims_report.csv\"),\n",
    "        \"unresolved_claims\" : os.path.join(input_dir, \"unresolved_claims_report.csv\"),\n",
    "        #\"fee_schedule\" : os.path.join(input_dir, \"fee_schedule.csv\"),\n",
    "        #\"openings\" : os.path.join(input_dir,\"openings.csv\"),\n",
    "        #\"schedule\" : os.path.join(input_dir,\"schedule.csv\"),\n",
    "        \"patient_list\" : os.path.join(input_dir, \"ZR - Patient List with Details.csv\"),\n",
    "        \"processed_payments\": os.path.join(input_dir, \"ZR - Credit Card Processed Payments.csv\"),\n",
    "        \"transaction_details\" : os.path.join(input_dir, \"ZR - Transaction Detail.csv\"),\n",
    "        \"treatment_tracker\" : os.path.join(input_dir, \"ZR - Treatment Tracker.csv\"),\n",
    "    }\n",
    "\n",
    " # Load datasets\n",
    "dataframes = {dataset: pd.read_csv(file_path) for dataset, file_path in csv_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataframes.keys():\n",
    "    print(d)\n",
    "    print(dataframes[d].columns)\n",
    "    print(\"\\n\")\n",
    "    print(dataframes[d].head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "dataframes['aged_AR']['Ascend Patient ID'] = dataframes['aged_AR']['id']\n",
    "dataframes['outstanding_claims']['Ascend Patient ID'] = dataframes['outstanding_claims']['patient.id']\n",
    "dataframes['statement_submission']['Ascend Patient ID'] = dataframes['statement_submission']['patient.id']\n",
    "\n",
    "\n",
    "for key in dataframes:\n",
    "    if key not in ['integrated_payments', 'unresolved_claims', 'processed_payments']:\n",
    "        print(key)\n",
    "        print(dataframes[key]['Ascend Patient ID'].head())\n",
    "        df_rows = dataframes[key]['Ascend Patient ID'].apply(lambda x: str(x).isnumeric() and 'Total' not in str(x))\n",
    "        print(df_rows.head())\n",
    "        dataframes[key] = dataframes[key].loc[df_rows, :]\n",
    "        print(dataframes[key].head())\n",
    "        dataframes[key]['merge_key'] = dataframes[key]['Ascend Patient ID'].astype('Int64').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data\n",
    "dataframes\n",
    "merged_df = dataframes['patient_list'].copy()\n",
    "for key in dataframes:\n",
    "        if key in ['statement_submission', 'aged_AR']:\n",
    "                print(key)\n",
    "                merged_df = merged_df.merge(dataframes[key], on='Ascend Patient ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(f\"{output_dir}/merged_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = defaultdict(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Aged AR:<br>\n",
    "- ~~Flag for \"remaining guarantor portion\"~~\n",
    "- ~~Total amount and distributions based on bucket - overlaid or grouped~~\n",
    "- ~~Wide-to-long~~\n",
    "- - Aging bucket, responsible party, amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['aged_AR'].copy()\n",
    "\n",
    "df_t['total_guarantorPortion'] = df_t.loc[:, ['before30.guarantorPortion', 'before60.guarantorPortion', 'before90.guarantorPortion', 'over90.guarantorPortion']].sum(axis=1)\n",
    "df_t['remaining_guarantorPortion'] = df_t['total_guarantorPortion'] > 0\n",
    "\n",
    "df_t['total_insurancePortion'] = df_t.loc[:, ['before30.insurancePortion', 'before60.insurancePortion', 'before90.insurancePortion', 'over90.insurancePortion']].sum(axis=1)\n",
    "df_t['remaining_insurancePortion'] = df_t['total_insurancePortion'] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide to Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t\n",
    "df_long = df_t.melt(id_vars=['id', 'Ascend Patient ID'], \n",
    "                    value_vars=['before30.amount', 'before60.amount', 'before90.amount', 'over90.amount',\n",
    "                                'before30.guarantorPortion', 'before60.guarantorPortion', 'before90.guarantorPortion', 'over90.guarantorPortion',\n",
    "                                'before30.insurancePortion', 'before60.insurancePortion', 'before90.insurancePortion', 'over90.insurancePortion'], \n",
    "                    var_name='Aging Bucket', \n",
    "                    value_name='Amount')\n",
    "\n",
    "df_long[['Bucket', 'Responsible Party']] = df_long['Aging Bucket'].str.split('.', expand=True)\n",
    "df_long['Responsible Party'] = df_long['Responsible Party'].str.replace('amount', 'Total').str.replace('guarantorPortion', 'Guarantor').str.replace('insurancePortion', 'Insurance')\n",
    "df_long['Bucket'] = df_long['Bucket'].str.replace('before30', '0-30').str.replace('before60', '31-60').str.replace('before90', '61-90').str.replace('over90', '91+')\n",
    "\n",
    "df_long = df_long.drop(columns=['Aging Bucket'])\n",
    "\n",
    "print(df_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['aged_AR'] = df_t.copy()\n",
    "transformed_data['aged_AR'].to_csv(f\"{output_dir}/transformed_aged_AR.csv\", index=False)\n",
    "\n",
    "transformed_data['aged_AR_long'] = df_long.copy()\n",
    "transformed_data['aged_AR_long'].to_csv(f\"{output_dir}/transformed_aged_AR_long.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Integrated Payments\n",
    "- ~~Parse transactionCardholderName to pull \"discover\", \"visa\", etc~~\n",
    "- ~~Check \"ZZ\" for transactionCardholderName~~\n",
    "- ~~Amount over time~~\n",
    "- ~~Pair and remove refunds and voids~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['integrated_payments'].copy()\n",
    "\n",
    "df_t.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rolling Sum\n",
    "\n",
    "# Ensure the transactionDateTime column is in datetime format\n",
    "df_t['transactionDateTime'] = convert_unix_timestamps(df_t,'transactionDateTime')\n",
    "\n",
    "# Set the transactionDateTime as the index\n",
    "df_t.set_index('transactionDateTime', inplace=True)\n",
    "\n",
    "# Calculate the rolling sum and count of transactions week-over-week\n",
    "df_t['rolling_sum'] = df_t['transactionAmount'].rolling('7D').sum()\n",
    "df_t['transaction_count'] = df_t['transactionAmount'].rolling('7D').count()\n",
    "\n",
    "df_t['event'] = df_t['ledgerType'].str.replace('CREDIT_CARD_', '').str.lower()\n",
    "\n",
    "# Reset the index\n",
    "df_t.reset_index(inplace=True)\n",
    "\n",
    "print(df_t[['transactionDateTime', 'transactionAmount', 'ledgerType', 'event', 'rolling_sum', 'transaction_count']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substring extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract card type from transactionCardholderName\n",
    "df_t['Card Type'] = df_t['transactionCardholderName'].str.extract(\n",
    "    '(visa|discover|mc|mastercard|amex|americanexpress)', \n",
    "    flags=re.IGNORECASE, \n",
    "    expand=False\n",
    ").str.lower()\n",
    "\n",
    "# Fill missing values with 'unknown'\n",
    "df_t['Card Type'] = df_t['Card Type'].fillna('unknown')\n",
    "\n",
    "print(df_t.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['integrated_payments'] = df_t.copy()\n",
    "transformed_data['integrated_payments'].to_csv(f\"{output_dir}/transformed_integrated_payments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Outstanding Claims\n",
    "- ~~Subscriber DoB > 20 years from Guarantor DoB~~\n",
    "- Everything by Insurer\n",
    "- ~~Parse and consolidate group plan name~~\n",
    "- Balance - Estimate?\n",
    "- ~~Flag for student plans~~\n",
    "- ~~- patient DoB vs subscriber DoB~~\n",
    "- ~~Aggregate insurance carriers that are state-specific~~\n",
    "- - Blue Cross Blue Shield\n",
    "- - Delta Dental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['outstanding_claims'].copy()\n",
    "\n",
    "df_t['student'] = df_t['student'] = (df_t['subscriber.firstName'] != df_t['patient.firstName']) & \\\n",
    "                  (pd.to_datetime(df_t['subscriber.dateOfBirth']) > pd.to_datetime(df_t['patient.dateOfBirth']) + pd.DateOffset(years=18))\n",
    "\n",
    "df_t['student'] = df_t['student'].replace({True: 'Student', False: 'Non-Student'})\n",
    "\n",
    "df_t.student.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['outstanding_claims'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['outstanding_claims'] = df_t.copy()\n",
    "transformed_data['outstanding_claims'].to_csv(f\"{output_dir}/transformed_outstanding_claims.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Patient Details\n",
    "- ~~Calculate Patient Lifespan~~\n",
    "- - ~~Today - First Visit and Active~~\n",
    "- - ~~Last Visit - First Visit~~\n",
    "- - ~~Next Appointment Date - First Visit~~\n",
    "- ~~Aggregate insurance carriers that are state-specific~~\n",
    "- ~~Year of Birth~~\n",
    "- ~~Month of Birth~~\n",
    "- ~~Last Visit != Last Procedure~~\n",
    "- ~~Remove test patients~~\n",
    "- ~~View uppercase and lowercase patients, empty values~~\n",
    "- ~~Patient != Guarantor~~\n",
    "- - ~~Flag for student plans~~\n",
    "- Parse and aggregate plans\n",
    "- - Awaiting client response\n",
    "- ~~Drop \"685806 - \" from Pat. Prim. Fee Schedule and Discount Plan~~\n",
    "- Parse address\n",
    "- ~~Group dates to month~~\n",
    "- ~~Flag who hasn't been in 6 months~~\n",
    "- ~~Flag for hasNextAppointment~~\n",
    "- - ~~and isn't in the past~~\n",
    "- Flag for Medicare/Medicaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['patient_list'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Patient'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows where Patient is all uppercase or all lowercase\n",
    "uppercase_patients = df_t[df_t['Patient'].str.isupper()]\n",
    "lowercase_patients = df_t[df_t['Patient'].str.islower()]\n",
    "\n",
    "# Combine the results\n",
    "filtered_patients = pd.concat([uppercase_patients, lowercase_patients])\n",
    "\n",
    "print(filtered_patients)\n",
    "# Isolate patients with \"test\" in the name or three or more repeated letters\n",
    "test_patients = df_t[df_t['Patient'].str.lower().str.contains(r'test', case=False, regex=True)]\n",
    "repeated_letter_patients = df_t[df_t['Patient'].str.contains(r'(.)\\1{2,}', case=False, regex=True)]\n",
    "\n",
    "# Combine the results\n",
    "patients_to_drop = pd.concat([test_patients, repeated_letter_patients]).drop_duplicates()\n",
    "\n",
    "print(patients_to_drop)\n",
    "\n",
    "# Drop the isolated patients from the original dataframe\n",
    "df_t = df_t[~df_t.index.isin(patients_to_drop.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['patientAge'] = (pd.to_datetime('today') - pd.to_datetime(df_t['Date Of Birth'])).dt.days // 365.25\n",
    "df_t['student'] = (df_t['Patient'] != df_t['Primary Guarantor']) & (df_t['patientAge'] < 25)\n",
    "\n",
    "df_t['student'] = df_t['student'].replace({True: 'Student', False: 'Non-Student'})\n",
    "\n",
    "df_t.student.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the date columns are in datetime format\n",
    "df_t['First Visit'] = pd.to_datetime(df_t['First Visit'])\n",
    "df_t['Last Visit'] = pd.to_datetime(df_t['Last Visit'])\n",
    "df_t['Next Appointment Date'] = pd.to_datetime(df_t['Next Appointment Date'])\n",
    "df_t['Last Procedure Date'] = pd.to_datetime(df_t['Last Procedure Date'])\n",
    "\n",
    "# Calculate the patient lifespan\n",
    "df_t['Lifespan (Today - First Visit)'] = (pd.to_datetime('today') - df_t['First Visit']).dt.days\n",
    "df_t['Lifespan (Last Visit - First Visit)'] = (df_t['Last Visit'] - df_t['First Visit']).dt.days\n",
    "df_t['Lifespan (Next Appointment Date - First Visit)'] = (df_t['Next Appointment Date'] - df_t['First Visit']).dt.days\n",
    "\n",
    "df_t['Time Since Last Visit'] = (pd.to_datetime('today') - df_t['Last Visit']).dt.days\n",
    "df_t['Visit and Procedure Mismatch'] = df_t['Last Visit'] != df_t['Last Procedure Date']\n",
    "\n",
    "df_t['hasNextAppointment'] = df_t['Next Appointment Date'].notnull() & (df_t['Next Appointment Date'] > pd.to_datetime('today'))\n",
    "df_t['overdue'] = (df_t['Time Since Last Visit'] > 183) & (df_t['hasNextAppointment'] == False)\n",
    "\n",
    "print(df_t[['First Visit', 'Last Visit', 'Last Procedure Date', 'Next Appointment Date', 'Lifespan (Today - First Visit)', 'Lifespan (Last Visit - First Visit)', 'Lifespan (Next Appointment Date - First Visit)', 'Time Since Last Visit',\n",
    "            'Visit and Procedure Mismatch', 'hasNextAppointment', 'overdue']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Pat. Prim. Fee Schedule'] = df_t['Pat. Prim. Fee Schedule'].str.replace('685806 - ', '')\n",
    "df_t['Discount Plan'] = df_t['Discount Plan'].str.replace('685806 - ', '')\n",
    "\n",
    "print(df_t[['Pat. Prim. Fee Schedule', 'Discount Plan']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['patient_list']['Pat. Prim. Carrier'].value_counts().head(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the carrier names\n",
    "def clean_carrier_name(carrier):\n",
    "    if pd.isnull(carrier):\n",
    "        return None\n",
    "    if isinstance(carrier, str):\n",
    "        if \"Blue Cross Blue Shield\" in carrier or \"BCBS\" in carrier:\n",
    "            return \"Blue Cross Blue Shield\"\n",
    "        elif \"Delta\" in carrier:\n",
    "            return \"Delta Dental\"\n",
    "        else:\n",
    "            return re.sub(r' of \\w+', '', carrier)\n",
    "    return carrier\n",
    "\n",
    "# Extract state information\n",
    "def extract_state(carrier):\n",
    "    if pd.isnull(carrier):\n",
    "        return None\n",
    "    if isinstance(carrier, str):\n",
    "        if \"Blue Cross Blue Shield\" or \"BCBS\" in carrier:\n",
    "            match = re.search(r' of (\\w+)', carrier)\n",
    "            if not match:\n",
    "                match = re.search(r'\\((\\w{2}(?: \\w{2})+)\\)', carrier)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "        elif \"Delta Dental\" in carrier:\n",
    "            match = re.search(r' of (\\w+)', carrier)\n",
    "        else:\n",
    "            return re.sub(r' of \\w+', '', carrier)\n",
    "        match = re.search(r' of (\\w+)', carrier)\n",
    "        return match.group(1) if match else None\n",
    "    return carrier\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df_t['Affiliate State'] = df_t['Pat. Prim. Carrier'].apply(extract_state)\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df_t['Cleaned Carrier'] = df_t['Pat. Prim. Carrier'].apply(clean_carrier_name)\n",
    "\n",
    "print(df_t[['Pat. Prim. Carrier', 'Cleaned Carrier', 'Affiliate State']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "import itertools\n",
    "\n",
    "#Extract unique values from the 'Pat. Prim. Plan' column\n",
    "plans = df_t['Pat. Prim. Plan'].dropna().unique()\n",
    "\n",
    "#Calculate similarity scores for all pairs\n",
    "similarity_scores = []\n",
    "for plan1, plan2 in itertools.combinations(plans, 2):\n",
    "    score = fuzz.ratio(plan1, plan2)\n",
    "    similarity_scores.append((plan1, plan2, score))\n",
    "\n",
    "#Sort the pairs by similarity score in descending order\n",
    "similarity_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "#Convert similarity scores to a DataFrame\n",
    "similarity_df = pd.DataFrame(similarity_scores, columns=['Plan 1', 'Plan 2', 'Similarity Score'])\n",
    "\n",
    "#Save the DataFrame to a CSV file\n",
    "similarity_df.to_csv(\"similarity_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the address data\n",
    "parsed_data = []\n",
    "\n",
    "addresses = df_t['Address']\n",
    "\n",
    "for address in addresses:\n",
    "    parts = [p.strip() for p in address.split(\",\")]  # Split by comma and remove extra spaces\n",
    "    \n",
    "    # Handle different address structures dynamically\n",
    "    street = parts[0] if len(parts) > 0 else \"\"\n",
    "    apartment = parts[1] if len(parts) > 3 else \"\"  # If there's a second part but before city/state\n",
    "    city = parts[-3].lower() if len(parts) > 2 else \"\"  # City is the third-to-last part\n",
    "    state = parts[-2] if len(parts) > 1 else \"\"  # State is the second-to-last part\n",
    "    zip_code = parts[-1] if len(parts) > 0 else \"\"  # ZIP is always last\n",
    "\n",
    "    parsed_data.append({\n",
    "        \"Street\": street,\n",
    "        #\"Apartment\": apartment,\n",
    "        \"City\": city,\n",
    "        \"State\": state,\n",
    "        \"ZIP Code\": zip_code\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "df_addresses = pd.DataFrame(parsed_data)\n",
    "\n",
    "df_addresses.head(20)\n",
    "# Add parsed columns back to the original dataframe\n",
    "df_t = pd.concat([df_t.reset_index(drop=True), df_addresses.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['DCS'] = df_t['Pat. Prim. Plan'].str.contains('DCS')\n",
    "df_t.groupby('Status')['DCS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['DNU'] = df_t['Pat. Prim. Plan'].str.contains('DNU')\n",
    "df_t.groupby('Status')['DNU'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate DCS and DNU columns\n",
    "# df_t['DCS_DNU'] = df_t['DCS'].astype(str) + '_' + df_t['DNU'].astype(str)\n",
    "\n",
    "# # Get the count of combinations for Status and DCS_DNU\n",
    "# status_dcs_dnu_counts = df_t.groupby(['Status', 'DCS_DNU']).size().reset_index(name='Count')\n",
    "\n",
    "# print(status_dcs_dnu_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plan_name(plan_name):\n",
    "    \"\"\"\n",
    "    Cleans a dental plan name by removing specified substrings and formatting it properly.\n",
    "\n",
    "    Args:\n",
    "        plan_name (str): The original plan name.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned plan name.\n",
    "    \"\"\"\n",
    "    if not isinstance(plan_name, str) or not plan_name.strip():\n",
    "        return plan_name  # Return as is if empty or not a string\n",
    "\n",
    "    # Substrings to remove (case insensitive)\n",
    "    remove_substrings = [\"inc\", \"llc\", \"dcs\", \"dnu\", \"corporation\", \"incorporated\", \"technologies\", \"healthcare\", \"international\", \"partners\", \"systems\", \"services\"]\n",
    "\n",
    "    # Remove specified substrings\n",
    "    for substring in remove_substrings:\n",
    "        plan_name = re.sub(rf\"\\b{substring}\\b\", \"\", plan_name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace non-alphanumeric characters with a space\n",
    "    plan_name = re.sub(r\"[^a-zA-Z0-9]\", \" \", plan_name)\n",
    "\n",
    "    # Trim multiple spaces down to a single space\n",
    "    plan_name = re.sub(r\"\\s+\", \" \", plan_name).strip()\n",
    "\n",
    "    return plan_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['plan_name'] = df_t['Pat. Prim. Plan'].apply(clean_plan_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_non_overlapping_parts(plan, cleaned_plan):\n",
    "    \"\"\"\n",
    "    Extract non-overlapping parts and words containing non-alphabetic characters from the original plan name.\n",
    "\n",
    "    Args:\n",
    "        plan (str): The original plan name.\n",
    "        cleaned_plan (str): The cleaned plan name.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the non-overlapping parts and words with non-alphabetic characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(plan, str) or not isinstance(cleaned_plan, str):\n",
    "        return plan  # Return as is if not a string\n",
    "\n",
    "    # Substrings to remove (case insensitive)\n",
    "    remove_substrings = [\"inc\", \"llc\", \"dcs\", \"dnu\", \"corporation\", \"incorporated\", \"technologies\", \"healthcare\", \"international\", \"partners\", \"systems\", \"services\"]\n",
    "    plan = re.sub(r'[.,-]', '', plan)  # Remove commas, periods, and hyphens\n",
    "\n",
    "    # Remove specified substrings\n",
    "    for substring in remove_substrings:\n",
    "        plan = re.sub(rf\"\\b{substring}\\b\", \"\", plan, flags=re.IGNORECASE)\n",
    "\n",
    "    # Split the original and cleaned plan names into words\n",
    "    plan_words = set(plan.split())\n",
    "    cleaned_plan_words = set(cleaned_plan.split())\n",
    "\n",
    "    # Find non-overlapping words\n",
    "    non_overlapping_words = plan_words - cleaned_plan_words\n",
    "\n",
    "    # Find words containing non-alphabetic characters\n",
    "    words_with_non_alpha = {word for word in plan_words if not word.isalpha()}\n",
    "\n",
    "    # Combine the results\n",
    "    result = non_overlapping_words.union(words_with_non_alpha)\n",
    "\n",
    "    # Remove empty strings, words with ending hyphens, and single characters\n",
    "\n",
    "    # Join the results and strip consecutive whitespace to single spaces\n",
    "    return ' '.join(result).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new column\n",
    "df_t['plan_name_cruft'] = df_t.apply(lambda row: extract_non_overlapping_parts(row['Pat. Prim. Plan'], row['plan_name']), axis=1)\n",
    "\n",
    "# Display the results\n",
    "print(df_t[['Pat. Prim. Plan', 'plan_name', 'plan_name_cruft']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['plan_name_cruft'].value_counts().head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_ind = df_t['plan_name_cruft'].str.contains(\"SCH/CAREERCTU\", regex=False).fillna(False)\n",
    "df_t.loc[test_case_ind,['Pat. Prim. Plan', 'plan_name', 'plan_name_cruft']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_ind = df_t['plan_name'].str.lower().str.contains(\"ass\", regex=False).fillna(False)\n",
    "df_t.loc[test_case_ind,['Pat. Prim. Plan', 'plan_name', 'plan_name_cruft']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_ind = df_t['Pat. Prim. Plan'].str.contains(\"LENOVO (UNITED STATES) INC. (2500)- DCS DNU\", regex=False).fillna(False)\n",
    "df_t.loc[test_case_ind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define invalid substring pairs (e.g., \"HMO\" should not match \"PPO\")\n",
    "INVALID_SUBSTRING_PAIRS = [\n",
    "    (\"HMO\", \"PPO\"), \n",
    "    (\"Advantage\", \"Medicaid\"), \n",
    "    (\"Basic\", \"Premium\"), \n",
    "    (\"Select\", \"Standard\"),\n",
    "    (\"Select\", \"Basic\"),\n",
    "    (\"(Self)\", \"(Self)\"),\n",
    "    (\"1A\", \"1B\"),\n",
    "    (\"Teacher\", \"Career\"),\n",
    "    ('Diamond', 'Core'),\n",
    "    ('City of', 'University of'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_common_substring(str1, str2):\n",
    "    \"\"\"\n",
    "    Find the longest common substring between two strings using dynamic programming.\n",
    "\n",
    "    Args:\n",
    "        str1 (str): First string.\n",
    "        str2 (str): Second string.\n",
    "\n",
    "    Returns:\n",
    "        int: Length of the longest common substring.\n",
    "    \"\"\"\n",
    "    str1, str2 = str1.lower(), str2.lower()\n",
    "    len1, len2 = len(str1), len(str2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "    max_length = 0\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                max_length = max(max_length, dp[i][j])\n",
    "\n",
    "    return max_length\n",
    "\n",
    "def substring_overlap_percentage(str1, str2):\n",
    "    \"\"\"\n",
    "    Compute the longest common substring overlap as a percentage of the shortest string.\n",
    "    Also includes a Levenshtein similarity check.\n",
    "\n",
    "    Args:\n",
    "        str1 (str): First string.\n",
    "        str2 (str): Second string.\n",
    "\n",
    "    Returns:\n",
    "        dict: Overlap percentage, Levenshtein similarity, and geometric mean.\n",
    "    \"\"\"\n",
    "    if not str1 or not str2:\n",
    "        return {\"overlap_percentage\": 0.0, \"levenshtein_similarity\": 0.0, \"geometric_mean\": 0.0}\n",
    "\n",
    "    str1, str2 = str1.lower(), str2.lower()\n",
    "    longest_substring = longest_common_substring(str1, str2)\n",
    "    min_length = min(len(str1), len(str2))\n",
    "\n",
    "    overlap_percentage = (longest_substring / min_length) * 100 if min_length > 0 else 0.0\n",
    "    levenshtein_sim = 1 - (Levenshtein.distance(str1, str2) / max(len(str1), len(str2)))\n",
    "\n",
    "    similarity_scores = [overlap_percentage, levenshtein_sim * 100]\n",
    "    geometric_mean_value = gmean(similarity_scores) if min(similarity_scores) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"overlap_percentage\": round(overlap_percentage, 2),\n",
    "        \"levenshtein_similarity\": round(levenshtein_sim * 100, 2),\n",
    "        \"geometric_mean\": round(geometric_mean_value, 2)\n",
    "    }\n",
    "\n",
    "def contains_invalid_substring_pair(str1, str2):\n",
    "    \"\"\"\n",
    "    Check if a string pair contains any invalid substring combinations.\n",
    "\n",
    "    Args:\n",
    "        str1 (str): First string.\n",
    "        str2 (str): Second string.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an invalid pair is found, otherwise False.\n",
    "    \"\"\"\n",
    "    for substr1, substr2 in INVALID_SUBSTRING_PAIRS:\n",
    "        if (substr1.lower() in str1.lower() and substr2.lower() in str2.lower()) or \\\n",
    "           (substr2.lower() in str1.lower() and substr1.lower() in str2.lower()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def batch_process_similarity(df, column_name):\n",
    "    \"\"\"\n",
    "    Compute substring overlap, Levenshtein similarity, and geometric mean for all unique string pairs.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a column of text values.\n",
    "        column_name (str): Column name containing the text data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with pairwise similarity scores and an invalid pair flag.\n",
    "    \"\"\"\n",
    "    unique_strings = df[column_name].dropna().unique()\n",
    "    string_pairs = list(combinations(unique_strings, 2))\n",
    "\n",
    "    results = []\n",
    "    for str1, str2 in string_pairs:\n",
    "        invalid_pair = contains_invalid_substring_pair(str1, str2)\n",
    "        scores = substring_overlap_percentage(str1, str2)\n",
    "        results.append({\n",
    "            \"string_1\": str1,\n",
    "            \"string_2\": str2,\n",
    "            **scores,\n",
    "            \"invalid_pair\": invalid_pair  # Flag invalid pairs instead of skipping them\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_results = batch_process_similarity(df_t, \"plan_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_results.sort_values(\"geometric_mean\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results\n",
    "similarity_results.to_csv(f\"{output_dir}/plan_similarity_results.csv\", index=False)\n",
    "\n",
    "# Display output\n",
    "print(similarity_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['university_or_college_plan'] = df_t['Pat. Prim. Plan'].str.contains('university' or 'college' or 'u of', case=False, na=False)\n",
    "df_t['university_or_college_plan'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate_plans = pd.read_csv(f\"{output_dir}/plan_consolidation_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Ensure columns are properly named (assuming first two columns represent equivalent plans)\n",
    "consolidate_plans.columns = [\"Plan_A\", \"Plan_B\"]\n",
    "\n",
    "# Create a graph to model the equivalence relationships\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(consolidate_plans.values)\n",
    "\n",
    "# Find connected components (each component represents a set of equivalent plans)\n",
    "components = list(nx.connected_components(G))\n",
    "\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from each plan to the simplest representation (sorted alphabetically within each component)\n",
    "equivalence_mapping = {}\n",
    "for component in components:\n",
    "    sorted_plans = sorted(component, key=lambda x: (len(x), x))  # Sort by length and then alphabetically\n",
    "    simplest_plan = sorted_plans[0].lower()  # Choose the first (simplest) plan name\n",
    "    for plan in component:\n",
    "        equivalence_mapping[plan] = simplest_plan\n",
    "\n",
    "# Convert the mapping into a DataFrame for easy inspection\n",
    "mapping_df = pd.DataFrame(list(equivalence_mapping.items()), columns=[\"Plan_Name\", \"Simplest_Equivalent\"])\n",
    "\n",
    "mapping_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Simplest_Equivalent_Plan'] = df_t['plan_name'].map(mapping_df.set_index('Plan_Name')['Simplest_Equivalent']).fillna(df_t['plan_name'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Simplest_Equivalent_Plan'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Pat. Prim. Plan'].value_counts().head(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new column\n",
    "df_t['temp'] = df_t['plan_name'].str.lower()\n",
    "df_t['plan_name_diff'] = df_t.apply(lambda row: extract_non_overlapping_parts(row['temp'], row['Simplest_Equivalent_Plan']), axis=1)\n",
    "\n",
    "df_t.drop(columns=['temp'], inplace=True)\n",
    "\n",
    "# Display the results\n",
    "print(df_t[['Pat. Prim. Plan', 'plan_name', 'DCS', 'DNU', 'Simplest_Equivalent_Plan', 'plan_name_diff']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['patient_details'] = df_t.copy()\n",
    "transformed_data['patient_details'].to_csv(f\"{output_dir}/transformed_patient_details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['active_patients'] = df_t.loc[df_t['Status'] == 'ACTIVE',:].copy()\n",
    "transformed_data['active_patients'].to_csv(f\"{output_dir}/transformed_active_patient_details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Processed Payments\n",
    "- ~~Break out and parse transaction types~~\n",
    "- ~~Parse Reference Number to pull out Aenta, DD, Cigna, etc~~\n",
    "- Split dataset into Insurance, Patient, Office, etc\n",
    "- Group by card type, insurance, etc\n",
    "- Amount by date\n",
    "- ~~Calculate differences between Not Available dates and other dates to get time to various metrics, like time to full payment~~\n",
    "- ~~Calculate running totals ~~\n",
    "- Maybe make dataset that's more of as point-in-time option\n",
    "- ~~Flip negative and positive~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['processed_payments'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Amount'] = df_t['Amount'].str.replace('$', '').str.replace(',', '').astype(float) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = df_t['Reference Number'].str.contains(r\"(?i)test\", na=False, regex=True)\n",
    "df_t[matches == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"test\" rows\n",
    "df_t = df_t[matches != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Date (Modified) column is in datetime format\n",
    "df_t['Date (Modified)'] = pd.to_datetime(df_t['Date (Modified)'])\n",
    "\n",
    "# Create a sort key for the Transaction Type\n",
    "df_t['sort_key'] = 1\n",
    "df_t.loc[df_t['Transaction Type'] == 'Not Available', 'sort_key'] = 0\n",
    "#df_t['sort_key'] = df_t['Transaction Type'].replace({'Not Available': 0, 'Payment': 1, 'Adjustment': 2, 'Refund': 3})\n",
    "\n",
    "# Sort the dataset by Ascend Patient ID and Date (Modified)\n",
    "df_t.sort_values(by=['Ascend Patient ID', 'Date (Modified)', 'sort_key'], inplace=True)\n",
    "\n",
    "# Calculate the difference in days between consecutive dates for each Ascend Patient ID\n",
    "df_t['Days Between'] = df_t.groupby('Ascend Patient ID')['Date (Modified)'].diff().dt.days\n",
    "# Initialize a new column for the rolling difference\n",
    "df_t['Rolling Days Between'] = np.nan\n",
    "\n",
    "# Iterate through each Ascend Patient ID\n",
    "for patient_id, group in df_t.groupby('Ascend Patient ID'):\n",
    "    # Sort the group by Date (Modified)\n",
    "    group = group.sort_values(by=['Date (Modified)', 'sort_key'])\n",
    "    \n",
    "    # Initialize the last incurred charge date\n",
    "    last_incurred_date = None\n",
    "    \n",
    "    # Iterate through each row in the group\n",
    "    for idx, row in group.iterrows():\n",
    "        if row['Transaction Type'] == 'Not Available':\n",
    "            # Update the last incurred charge date\n",
    "            last_incurred_date = row['Date (Modified)']\n",
    "            df_t.loc[idx, 'Rolling Days Between'] = 0\n",
    "        elif last_incurred_date is not None:\n",
    "            # Calculate the difference in days from the last incurred charge date\n",
    "            df_t.loc[idx, 'Rolling Days Between'] = (row['Date (Modified)'] - last_incurred_date).days\n",
    "        print(last_incurred_date)\n",
    "\n",
    "# Filter the dataset to calculate the days between dates for \"Transaction Type\" = \"Not Available\" and other transaction types\n",
    "df_not_available = df_t[df_t['Transaction Type'] == 'Not Available']\n",
    "df_other = df_t[df_t['Transaction Type'] != 'Not Available']\n",
    "\n",
    "# Display the results\n",
    "#print(df_not_available[['Ascend Patient ID', 'Transaction Type', 'Date (Modified)', 'Days Between', 'Rolling Days Between']])\n",
    "#print(df_other[['Ascend Patient ID', 'Transaction Type', 'Date (Modified)', 'Days Between', 'Rolling Days Between']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_t['Transaction Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t[['Ascend Patient ID', 'Transaction Type', 'Date (Modified)', 'Days Between', 'Rolling Days Between']].head(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Amount column is in numeric format\n",
    "df_t['Amount'] = df_t['Amount'].astype(float)\n",
    "\n",
    "# Calculate the running total for Amount by each Ascend Patient ID\n",
    "df_t['Running Total'] = df_t.groupby('Ascend Patient ID')['Amount'].cumsum()\n",
    "\n",
    "# Display the results\n",
    "print(df_t[['Ascend Patient ID', 'Transaction Type', 'Date (Modified)', 'Amount', 'Running Total']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = df_t.copy()\n",
    "df_t2 = df_t.copy()\n",
    "\n",
    "df_t1 = df_t1.loc[df_t1['Transaction Type'] != 'Not Available']\n",
    "df_t2 = df_t2.loc[df_t2['Transaction Type'] == 'Not Available']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract card type from transactionCardholderName\n",
    "df_t1['Extracted Card Type'] = df_t1['Card Holder'].str.extract(\n",
    "    '(visa|discover|mc|mastercard|amex|americanexpress)', \n",
    "    flags=re.IGNORECASE, \n",
    "    expand=False\n",
    ").str.lower()\n",
    "\n",
    "# Fill missing values with 'unknown'\n",
    "df_t1['Extracted Card Type'] = df_t1['Extracted Card Type'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframes['patient_list']['Pat. Prim. Carrier'].value_counts().head(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract card type from transactionCardholderName\n",
    "#test = df_t1['Reference Number'].str.replace(r'\\d+', '', regex=True).str.lower()\n",
    "\n",
    "#test.value_counts().head(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract card type from transactionCardholderName\n",
    "df_t1['Extracted Insurance'] = df_t1['Reference Number'].str.replace(r'\\d+', '', regex=True).str.lower().str.extract(\n",
    "    r'^(cigna|dd|mc|aetna|ml|uhc|sl|guardian|geha)', \n",
    "    flags=re.IGNORECASE, \n",
    "    expand=False\n",
    ")\n",
    "\n",
    "# Fill missing values with 'unknown'\n",
    "df_t1['Extracted Insurance'] = df_t1['Extracted Insurance'].fillna('unknown')\n",
    "\n",
    "df_t1['Extracted Insurance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['processed_payments'] = df_t.copy()\n",
    "transformed_data['payments'] = df_t1.copy()\n",
    "transformed_data['incurred_charges'] = df_t2.copy()\n",
    "\n",
    "transformed_data['processed_payments'].to_csv(f\"{output_dir}/transformed_processed_payments.csv\", index=False)\n",
    "transformed_data['payments'].to_csv(f\"{output_dir}/transformed_payments.csv\", index=False)\n",
    "transformed_data['incurred_charges'].to_csv(f\"{output_dir}/transformed_incurred_charges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Statement Submissions\n",
    "- ~~Drop or flag negative balance~~\n",
    "- Grouping by generatedFrom, source, statement type, month\n",
    "- Balance over time total\n",
    "- ~~Balance over time - patient~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['statement_submission'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (df_t['patient.firstName'] + \", \" + df_t['patient.lastName']).str.contains(r\"(?i)test\", na=False, regex=True)\n",
    "df_t[matches == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"test\" rows\n",
    "df_t = df_t[matches != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['dateTime'] = convert_unix_timestamps(df_t, 'dateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['neg_balances'] = df_t['balance']<0\n",
    "df_t.loc[df_t['neg_balances'],:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the balance column is in numeric format\n",
    "df_t['balance'] = pd.to_numeric(df_t['balance'], errors='coerce')\n",
    "\n",
    "# Ensure the patient.id column is in string format\n",
    "df_t['patient.id'] = df_t['patient.id'].astype(str)\n",
    "\n",
    "# Ensure the dateTime column is in datetime format\n",
    "df_t['dateTime'] = pd.to_datetime(df_t['dateTime'], errors='coerce')\n",
    "\n",
    "# Sort the dataframe by patient.id and dateTime\n",
    "df_t = df_t.sort_values(by=['patient.id', 'dateTime'])\n",
    "\n",
    "# Calculate the marginal difference between adjacent rows on the \"balance\" column\n",
    "df_t['balance_diff'] = df_t.groupby('patient.id')['balance'].diff()\n",
    "\n",
    "# Display the results\n",
    "print(df_t[['patient.id', 'dateTime', 'balance', 'balance_diff']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the dateTime column is in datetime format\n",
    "df_t['dateTime'] = pd.to_datetime(df_t['dateTime'], errors='coerce')\n",
    "\n",
    "# Sort the dataframe by patient.id and dateTime\n",
    "df_t = df_t.sort_values(by=['patient.id', 'dateTime'])\n",
    "\n",
    "# Add a column for possible duplicates based on adjacent dates\n",
    "df_t['possibleDuplicate'] = df_t.duplicated(subset=['patient.id', 'balance'], keep=False) & \\\n",
    "                            df_t.duplicated(subset=['patient.id', 'balance'], keep='last').shift(1).fillna(False)\n",
    "\n",
    "# Display the results\n",
    "print(df_t.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['statement_submission'] = df_t.copy()\n",
    "transformed_data['statement_submission'].to_csv(f\"{output_dir}/transformed_statement_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations for Transaction Details\n",
    "- Charge by Category\n",
    "- Categorical Comparisons\n",
    "- Charges by date\n",
    "- Possibly split based on Category\n",
    "- ~~Flag for referrals based on Proc Code~~\n",
    "- ~~Parse substrings on Proc Description~~\n",
    "- ~~Flag negative Charges~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['transaction_details'].copy()\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Charges'] = treat_currency(df_t, 'Charges')\n",
    "df_t['Credits'] = treat_currency(df_t, 'Credits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['neg_charges'] = df_t['Charges']<0\n",
    "df_t.loc[df_t['neg_charges'],:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['neg_charges'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['pos_credits'] = df_t['Credits']>0\n",
    "df_t.loc[df_t['pos_credits'],:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['pos_credits'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Extract all words from the 'Proc Description' column\n",
    "words = df_t['Proc. Description'].str.lower().str.split().explode()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Find common substrings of whole words\n",
    "common_words = {word: count for word, count in word_counts.items() if count > 1}\n",
    "\n",
    "# Sort the common words by their frequency in descending order\n",
    "sorted_common_words = dict(sorted(common_words.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(sorted_common_words)\n",
    "# Plot the top 24 words\n",
    "top_24_words = dict(itertools.islice({word: count for word, count in sorted_common_words.items() if len(word) > 4}.items(), 24))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(top_24_words.keys(), top_24_words.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 24 Most Common Words in Procedure Descriptions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = ['periodontal', 'evaluation', 'intraoral', 'periapical', 'prophylaxis', 'bitewing', 'resin', 'composite', 'porcelain', 'ceramic', 'images', 'arch', 'guard', 'gingival', 'fluoride', 'root', 'crown', 'buildup',\n",
    "               'metal', 'replacement', 'noble', 'upper', 'lower', 'denture', 'retainer', 'medicaments','palliative', 'late', 'cancellation', 'orthodontic', 'extraction', 'posterior','anterior', 'panoramic', 'radiographic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count columns for each word in the word list\n",
    "for word in phrase_list:\n",
    "    df_t[word] = df_t['Proc. Description'].str.contains(word, case=False, regex=False).astype(bool)\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(df_t.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "df_t_corr = df_t.copy()\n",
    "df_t_corr = df_t_corr.drop(columns=['Charges', 'Credits', 'neg_charges', 'pos_credits', 'Patient', 'Proc. Description', 'Date', 'Ascend Patient ID', 'Chart Number', 'merge_key'])\n",
    "\n",
    "df_t_corr_num = df_t_corr.select_dtypes(include=[np.number]).copy()\n",
    "correlation_matrix = df_t_corr_num.corr()\n",
    "\n",
    "# Find pairs of columns that are perfectly correlated\n",
    "perfect_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if correlation_matrix.iloc[i, j] == 1.0:\n",
    "            colname1 = correlation_matrix.columns[i]\n",
    "            colname2 = correlation_matrix.columns[j]\n",
    "            perfect_correlations.append((colname1, colname2))\n",
    "    # Check for perfect binary or categorical relationships\n",
    "for column in df_t_corr.select_dtypes(include=['object', 'category', 'bool']).columns:\n",
    "    unique_values = df_t_corr[column].nunique()\n",
    "    if unique_values == 2:\n",
    "        for other_column in df_t_corr.select_dtypes(include=['object', 'category', 'bool']).columns:\n",
    "            if column != other_column and df_t_corr.groupby(column)[other_column].nunique().max() == 1:\n",
    "                perfect_correlations.append((column, other_column))\n",
    "\n",
    "    # Print the pairs of perfectly correlated columns including binary or categorical relationships\n",
    "    print(\"Perfectly correlated column pairs including binary or categorical relationships:\")\n",
    "    for pair in perfect_correlations:\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['Referral'] = df_t['Proc. Code'].str.startswith('REF')\n",
    "df_t['Referral'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['transaction_details'] = df_t.copy()\n",
    "transformed_data['transaction_details'].to_csv(f\"{output_dir}/transformed_transaction_details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Treatment Tracker\n",
    "- Add month grouping\n",
    "- Remove test patients\n",
    "- Remove canceled and/or invalidated (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['treatment_tracker'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['treatment_tracker'] = df_t.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for Unresolved Claims\n",
    "- Plans by carrier grouping\n",
    "- Parse StateId\n",
    "- Group by: Carrier, Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = dataframes['unresolved_claims'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "transformed_data['unresolved_claims'] = df_t.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data\n",
    "transformed_data\n",
    "merged_df = transformed_data['patient_details'].copy()\n",
    "for key in transformed_data:\n",
    "        if key in ['statement_submission']:\n",
    "                print(key)\n",
    "                merged_df = merged_df.merge(transformed_data[key], on='Ascend Patient ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(f\"{output_dir}/merged_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['patient_details'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
